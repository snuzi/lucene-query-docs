are many images). We will use this function in the next section.

PCA of images

Principal Component Analysis (PCA) is a useful technique for dimensionality reduction
and is optimal in the sense that it represents the variability of the training data with
as few dimensions as possible. Even a tiny 100 x 100 pixel grayscale image has 10,000
dimensions, and can be considered a point in a 10,000 dimensional space. A megapixel
image has dimensions in the millions. With such high dimensionality, it is no surprise
that dimensionality reduction comes handy in many computer vision applications. The
projection matrix resulting from PCA can be seen as a change of coordinates to a
coordinate system where the coordinates are in descending order of importance.

To apply PCA on image data, the images need to be converted to a one-dimensional
vector representation, for example using NumPy’s f1atten() method.

The ﬂattened images are collected in a single matrix by stacking them, one row
for each image. The rows are then centered relative to the mean image before the
computation of the dominant directions. To ﬁnd the principal components, singular
value decomposition (SVD) is usually used, but if the dimensionality is high, there is a
useful trick that can be used instead since the SVD computation will be very slow in
that case. Here is what it looks like in code.

from PIL import Image
from numpy import *

def pca(X):
”"“ Principal Component Analysis
input: X, matrix with training data stored as flattened arrays in rows
return: projection matrix (with important dimensions first), variance and mean.

# get dimensions
num_data,dim = X.shape

# center data
mean_X = X.mean(axis=0)
X = X — mean,X

if dim>num,data:

# PCA - compact trick used

M = dot(X,X.T) # covariance matrix

e,EV = linalg.eigh(M) # eigenvalues and eigenvectors

tmp = dot(X.T,EV).T # this is the compact trick

V = tmp[::-1] # reverse since last eigenvectors are the ones we want
S sqrt(e)[::-1] # reverse since eigenvalues are in increasing order

1.3.NumPy 27

