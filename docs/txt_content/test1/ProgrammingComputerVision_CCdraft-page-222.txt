Using PCA to reduce dimensions

Now, let's try the gesture recognition problem. Since the feature vectors are very
large for the dense SIFT descriptor (more than 10000 for the parameter choices in the
example above) it is a good idea to do dimensionality reduction before ﬁtting models
to the data. Principal Component Analysis, PCA, (see Section 1.3) usually does a good
job. Try the following script that uses PCA from the ﬁle pca.py (page 28):

import pca
V,S,m = pca.pca(features)

# keep most important dimensions

V = V[:50]

features = array([d0t(V,f-m) for f in features])
testifeatures = array([dot(V,f-m) for f in test_features])

Here features and test_features are the same arrays that we loaded for the kNN exam-
ple. In this case we apply PCA on the training data and keep the 50 dimensions with
most variance. This is done by subtracting the mean 111 (computed on the training
data) and multiplying with the basis vectors V. The same transformation is applied to
the test data.
Train and test the Bayes classiﬁer like this:
# test Bayes

bc = bayes.BayesC1assifier()
blist = [features[where(1abe1s==c)[0]] for c in classnames]

bc.train(b1ist,classnames)
res = bc.c1assify(test_features)[0]

Since BayesClassifier takes a list of arrays (one array for each class) we transform
the data before passing it to the train() method. Since we don't need the probabilities
for now we chose to return only the labels of the classiﬁcation.

Checking the accuracy

acc = sum(1.0*(res==test_1abe1s)) / 1en(test_1abe1s)
print 'Accuracy:’, acc

gives something like this

Accuracy: 0.717277486911

and checking the confusion matrix

print_confusion(res,test,labe1s,c1assnames)

gives a print out like this:

222 8.2. Bayes Classiﬁer

